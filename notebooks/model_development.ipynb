{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe706e77",
   "metadata": {},
   "source": [
    "# Development of brain-fine-tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bceda",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33d79ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5b2e2",
   "metadata": {},
   "source": [
    "# Pre-processing for Harry Potter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678cd93",
   "metadata": {},
   "source": [
    "## ONLY FOR SUBJECT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87f6c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "harry_potter = loadmat('/home/ubuntu/NLP-brain-biased-robustness/data/harry_potter_brain/subject_1.mat')\n",
    "\n",
    "\n",
    "words = []\n",
    "for i in range(5176):\n",
    "    word = harry_potter['words'][0][i][0][0][0][0]\n",
    "    words.append(word)\n",
    "\n",
    "word_times = []\n",
    "for i in range(5176):\n",
    "    word_time = harry_potter['words'][0][i][1][0][0]\n",
    "    word_times.append(word_time)\n",
    "\n",
    "tr_times = []\n",
    "for i in range(1351):\n",
    "    tr_time = harry_potter['time'][i,0]\n",
    "    tr_times.append(tr_time)\n",
    "\n",
    "dont_include_indices = [i for i in range(15)] + [i for i in range(335,355)] + [i for i in range(687,707)] + [i for i in range(966,986)] + [i for i in range(1346,1351)]\n",
    "\n",
    "X_fmri = harry_potter['data']\n",
    "\n",
    "useful_X_fmri = np.delete(X_fmri, dont_include_indices,axis=0)\n",
    "\n",
    "tr_times_arr = np.asarray(tr_times)\n",
    "\n",
    "useful_tr_times = np.delete(tr_times_arr, dont_include_indices)\n",
    "\n",
    "sentences = [[]]*1271\n",
    "for idx, useful_tr_time in enumerate(useful_tr_times):\n",
    "    sentence= []\n",
    "    for word, word_time in zip(words,word_times):\n",
    "        if useful_tr_time - 10 <= word_time <= useful_tr_time:\n",
    "            sentence.append(word)\n",
    "    sentences[idx] = sentence   \n",
    "    \n",
    "\n",
    "actual_sentences = ['']*1271\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    for word in sentence:\n",
    "        actual_sentences[idx] = actual_sentences[idx] + word + ' '\n",
    "        \n",
    "\n",
    "fmri = torch.as_tensor(useful_X_fmri)\n",
    "truth_fmri = fmri[:5,:]\n",
    "truth_fmri.shape\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = []\n",
    "for i in range(1271):\n",
    "    dataset.append((actual_sentences[i], fmri[i,:]))\n",
    "    \n",
    "#TRAIN TEST SPLIT HAS OVERLAP IN WORDS AND IN BRAIN STATE\n",
    "n_rows = len(dataset)\n",
    "train_dataset = dataset[:int(.7*n_rows)]\n",
    "val_dataset = dataset[int(.8*n_rows):]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a0a63",
   "metadata": {},
   "source": [
    "# Pre-processing for parcellated NSD (26 dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a4650a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from pycocotools.coco import COCO\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "data_path = '/home/ubuntu/NLP-brain-biased-robustness/NSD/'\n",
    "\n",
    "coco3 = COCO(data_path+'annotations/captions_train2017.json')\n",
    "coco4 = COCO(data_path+'annotations/captions_val2017.json')\n",
    "\n",
    "def load_csv(csv_file):\n",
    "    file = open(csv_file)\n",
    "    csvreader = csv.reader(file)\n",
    "    header = next(csvreader)\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "    file.close()\n",
    "    return rows\n",
    "\n",
    "nsd_to_coco = load_csv(data_path+'nsd_stim_info_merged.csv')\n",
    "exp_design = scipy.io.loadmat(data_path+'nsd_expdesign.mat')\n",
    "ordering = exp_design['masterordering'].flatten() - 1 #fix indexing\n",
    "\n",
    "data_size = 22500 #trials[subject-1] #can use more than 22500 trials if seems promising\n",
    "ordering_data = ordering[:data_size]\n",
    "subjectim = exp_design['subjectim'] - 1\n",
    "\n",
    "def index_to_captions(my_index, subject):\n",
    "    index = ordering_data[my_index]\n",
    "    nsd_id = subjectim[subject-1,index]\n",
    "    coco_id = nsd_to_coco[nsd_id][1]\n",
    "    if int(nsd_id) < 2950:\n",
    "        annotation_ids = coco4.getAnnIds(int(coco_id))\n",
    "        annotations = coco4.loadAnns(annotation_ids)\n",
    "    else:\n",
    "        annotation_ids = coco3.getAnnIds(int(coco_id))\n",
    "        annotations = coco3.loadAnns(annotation_ids)\n",
    "    captions = [item['caption'] for item in annotations]\n",
    "    return captions\n",
    "\n",
    "NSD_fmri_parcellated = np.empty((22500,23,8))\n",
    "for subject in range(8):\n",
    "    X = scipy.io.loadmat(data_path+'X'+str(subject+1)+'.mat')\n",
    "    NSD_fmri_parcellated[:,:,subject] = X['X']\n",
    "    \n",
    "\n",
    "dataset = []\n",
    "for subject in range(8):\n",
    "    for my_index in range(22500):\n",
    "        descriptions = index_to_captions(my_index, subject+1)\n",
    "        brain_vec = NSD_fmri_parcellated[my_index,:,subject]\n",
    "        for description in descriptions:\n",
    "            example = (description, brain_vec)\n",
    "            dataset.append(example)\n",
    "\n",
    "#dataset is a list of ('sentence',23-dim numpy brain vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30899a10",
   "metadata": {},
   "source": [
    "# Model - BrainBiasedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b59243be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainBiasedBERT(nn.Module):\n",
    "    def __init__(self, num_voxels=37913):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.linear = nn.Linear(768,num_voxels)\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def forward(self, x):\n",
    "        embeddings = self.tokenizer(x, return_tensors='pt', padding=True)\n",
    "        embeddings.to(self.device)\n",
    "        representations = self.bert(**embeddings).last_hidden_state\n",
    "        cls_representation = representations[:,0,:]\n",
    "        pred_fmri = self.linear(cls_representation)\n",
    "        return pred_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1277f480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2p7nt8gt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▅▄▅▇▆█▅▆▆█▄▆▆▆▆▅▆▅▅▆▆▃▄▅▅▄▄▅▃▅▅▇▄▁▄▃▄▂▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>309724.34375</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fluent-hill-4</strong>: <a href=\"https://wandb.ai/nlp-brain-biased-robustness/preliminary%20results%20just%20in%20case/runs/2p7nt8gt\" target=\"_blank\">https://wandb.ai/nlp-brain-biased-robustness/preliminary%20results%20just%20in%20case/runs/2p7nt8gt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_062824-2p7nt8gt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2p7nt8gt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/NLP-brain-biased-robustness/notebooks/wandb/run-20220509_063004-1fp5cl53</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp-brain-biased-robustness/preliminary%20results%20just%20in%20case/runs/1fp5cl53\" target=\"_blank\">laced-bee-5</a></strong> to <a href=\"https://wandb.ai/nlp-brain-biased-robustness/preliminary%20results%20just%20in%20case\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/apis/normalize.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py\", line 1434, in upload_urls\n",
      "    raise CommError(f\"Run does not exist {entity}/{project}/{run_id}.\")\n",
      "wandb.errors.CommError: Run does not exist nlp-brain-biased-robustness/preliminary results just in case/1fp5cl53.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/filesync/upload_job.py\", line 56, in run\n",
      "    success = self.push()\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/filesync/upload_job.py\", line 107, in push\n",
      "    _, upload_headers, result = self._api.upload_urls(project, [self.save_name])\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/apis/normalize.py\", line 58, in wrapper\n",
      "    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/apis/normalize.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/environments/my_env/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py\", line 1434, in upload_urls\n",
      "    raise CommError(f\"Run does not exist {entity}/{project}/{run_id}.\")\n",
      "wandb.errors.CommError: Run does not exist nlp-brain-biased-robustness/preliminary results just in case/1fp5cl53.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"preliminary results just in case\", entity=\"nlp-brain-biased-robustness\")\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 5e-5,\n",
    "  \"epochs\": 15,\n",
    "  \"batch_size\": 8\n",
    "}\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        for batch in dataloader:\n",
    "            preds = model(list(batch[0]))\n",
    "            labels = batch[1].to(device)\n",
    "            test_loss = loss_function(preds, labels.float())\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "    return torch.mean(torch.as_tensor(test_losses)) \n",
    "\n",
    "    \n",
    "def train(model, dataloader, num_epochs=15): \n",
    "    last_val_loss = 9223372036854775807\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    num_training_steps = num_epochs * len(dataloader)\n",
    "    lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            preds = model(list(batch[0]))\n",
    "            labels = batch[1].to(device)\n",
    "            loss = loss_function(preds, labels.float()) #replace .loss\n",
    "            loss.backward()\n",
    "            \n",
    "            wandb.log({\"loss\": loss})\n",
    "            wandb.watch(model)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        val_loss = evaluate(model, test_dataloader)\n",
    "        wandb.log({\"training loss\": loss})\n",
    "        wandb.log({\"val loss\": val_loss})\n",
    "        if val_loss > last_val_loss:\n",
    "            print('Stopped early')\n",
    "            torch.save(model.state_dict(), 'fine_tuned_model')\n",
    "            break\n",
    "        last_val_loss = val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6396678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a8d233580c452d99dbef639407ecc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Summary data exceeds maximum size of 10.4MB. Dropping it.\n"
     ]
    }
   ],
   "source": [
    "model = BrainBiasedBERT()\n",
    "train(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93e608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fine_tuned_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
