{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electric-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpbbb as bbb\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0999f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"experiment\":{\n",
    "        \"lr\": [0.0001],\n",
    "        \"epochs\": [100],\n",
    "        \"batchsize\": [8],\n",
    "        \"val_frequency\": [5]\n",
    "    },\n",
    "    \"model\":{\n",
    "        \"brain_biased\": [False]\n",
    "    },\n",
    "    \"misc\": {\n",
    "        \"save\": [False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c614f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\"results/models/05.16.22/model_type:BrainBiasedBert$lr:5e-05/epoch:16/train_info\",\n",
    "#                      \"results/models/05.16.22/model_type:BrainBiasedBert$lr:5e-05/epoch:8/train_info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7756db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_exps = [\"Amazon_BABY\", \"MNLI_DEFAULT\", \"SST2_DEFAULT2\", \"Yelp_ITALIAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97e9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_dicts = bbb.setup.create_gridsearch(settings, default_name=\"HarryPotter_DEFAULT\", merge_default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa0606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:Amazon_BABY~lr:0.0001~epochs:100~batchsize:8~val_frequency:5~brain_biased:False\n",
      "config:MNLI_DEFAULT~lr:0.0001~epochs:100~batchsize:8~val_frequency:5~brain_biased:False\n",
      "config:SST2_DEFAULT2~lr:0.0001~epochs:100~batchsize:8~val_frequency:5~brain_biased:False\n",
      "config:Yelp_ITALIAN~lr:0.0001~epochs:100~batchsize:8~val_frequency:5~brain_biased:False\n"
     ]
    }
   ],
   "source": [
    "run_dicts = []\n",
    "for be in base_exps:\n",
    "    config_dicts = bbb.setup.create_gridsearch(settings, default_name=be, merge_default=True)\n",
    "    for cd in config_dicts:\n",
    "        run_dicts.append(cd)\n",
    "    #bbb.setup.run_submitit_job_array(config_dicts, timeout=(12 * 60), mem=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3223bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint.pprint(run_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903a87d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Training Epoch 1/100:   0%|           | 8/8000 [00:00<06:56, 19.20batch/s, train loss (batch)=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1764, 0.2140, 0.1663, 0.2627, 0.1806],\n",
      "        [0.1792, 0.2553, 0.1750, 0.2200, 0.1705],\n",
      "        [0.1669, 0.2413, 0.1444, 0.2669, 0.1805],\n",
      "        [0.2319, 0.1938, 0.1468, 0.2562, 0.1713],\n",
      "        [0.2054, 0.2145, 0.1553, 0.2101, 0.2147],\n",
      "        [0.1763, 0.2170, 0.1645, 0.2533, 0.1889],\n",
      "        [0.2023, 0.2388, 0.1309, 0.2442, 0.1837],\n",
      "        [0.1591, 0.2050, 0.1589, 0.2781, 0.1989]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   0%|         | 16/8000 [00:00<04:40, 28.46batch/s, train loss (batch)=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2248, 0.1829, 0.1491, 0.1244, 0.3189],\n",
      "        [0.2238, 0.1449, 0.1475, 0.1510, 0.3328],\n",
      "        [0.2272, 0.2134, 0.1478, 0.1104, 0.3012],\n",
      "        [0.2033, 0.1488, 0.1554, 0.1415, 0.3510],\n",
      "        [0.2512, 0.1708, 0.1796, 0.1299, 0.2686],\n",
      "        [0.2742, 0.1512, 0.1482, 0.1342, 0.2922],\n",
      "        [0.2546, 0.1406, 0.1673, 0.1389, 0.2987],\n",
      "        [0.2858, 0.1488, 0.1372, 0.1122, 0.3161]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   0%|         | 24/8000 [00:00<04:31, 29.37batch/s, train loss (batch)=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1266, 0.1200, 0.0832, 0.0649, 0.6053],\n",
      "        [0.0860, 0.0939, 0.0799, 0.0653, 0.6749],\n",
      "        [0.1473, 0.1248, 0.0921, 0.0996, 0.5362],\n",
      "        [0.0887, 0.1466, 0.0765, 0.0815, 0.6068],\n",
      "        [0.1126, 0.0827, 0.0736, 0.0602, 0.6710],\n",
      "        [0.0737, 0.0946, 0.0848, 0.0604, 0.6865],\n",
      "        [0.1013, 0.1158, 0.0879, 0.0888, 0.6062],\n",
      "        [0.0981, 0.0963, 0.0747, 0.0541, 0.6769]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   0%|          | 32/8000 [00:01<04:33, 29.10batch/s, train loss (batch)=0.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1633, 0.1344, 0.0918, 0.1138, 0.4967],\n",
      "        [0.1217, 0.1182, 0.0998, 0.1068, 0.5534],\n",
      "        [0.1609, 0.1489, 0.0919, 0.1076, 0.4907],\n",
      "        [0.1017, 0.0790, 0.0748, 0.0857, 0.6589],\n",
      "        [0.1277, 0.1529, 0.0913, 0.0749, 0.5531],\n",
      "        [0.1432, 0.1677, 0.0945, 0.0892, 0.5054],\n",
      "        [0.1502, 0.1258, 0.1055, 0.0977, 0.5207],\n",
      "        [0.1304, 0.0963, 0.0686, 0.0859, 0.6188]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n",
      "tensor([[0.1943, 0.1699, 0.1216, 0.1721, 0.3420],\n",
      "        [0.1478, 0.1179, 0.0946, 0.1220, 0.5177],\n",
      "        [0.1847, 0.1605, 0.1211, 0.1620, 0.3716],\n",
      "        [0.1610, 0.1307, 0.1072, 0.1461, 0.4549],\n",
      "        [0.2379, 0.1647, 0.1233, 0.1437, 0.3304],\n",
      "        [0.1945, 0.1332, 0.0978, 0.1511, 0.4234],\n",
      "        [0.2336, 0.1885, 0.1216, 0.1538, 0.3025],\n",
      "        [0.1501, 0.1183, 0.1147, 0.1614, 0.4555]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|         | 48/8000 [00:01<04:34, 28.95batch/s, train loss (batch)=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2362, 0.1406, 0.1287, 0.1661, 0.3283],\n",
      "        [0.1460, 0.1336, 0.1375, 0.1599, 0.4230],\n",
      "        [0.2023, 0.1542, 0.1301, 0.1534, 0.3600],\n",
      "        [0.1971, 0.1601, 0.1220, 0.2006, 0.3202],\n",
      "        [0.1794, 0.1128, 0.1235, 0.1692, 0.4151],\n",
      "        [0.2153, 0.1361, 0.1075, 0.1623, 0.3790],\n",
      "        [0.2106, 0.1187, 0.1151, 0.1851, 0.3705],\n",
      "        [0.2783, 0.1252, 0.1252, 0.1737, 0.2976]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|         | 56/8000 [00:01<04:16, 30.92batch/s, train loss (batch)=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1404, 0.1451, 0.1259, 0.1919, 0.3967],\n",
      "        [0.1945, 0.1137, 0.1517, 0.2424, 0.2977],\n",
      "        [0.2292, 0.1562, 0.1257, 0.1928, 0.2961],\n",
      "        [0.2612, 0.1520, 0.1283, 0.2013, 0.2571],\n",
      "        [0.2190, 0.1707, 0.1239, 0.2032, 0.2832],\n",
      "        [0.1996, 0.1558, 0.1250, 0.2002, 0.3193],\n",
      "        [0.1461, 0.1230, 0.1263, 0.1792, 0.4254],\n",
      "        [0.2294, 0.1562, 0.1342, 0.2056, 0.2746]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|          | 64/8000 [00:02<04:00, 33.02batch/s, train loss (batch)=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2076, 0.1556, 0.1440, 0.2294, 0.2633],\n",
      "        [0.1228, 0.1007, 0.0955, 0.1956, 0.4855],\n",
      "        [0.1838, 0.1356, 0.1312, 0.2169, 0.3326],\n",
      "        [0.1775, 0.1346, 0.1346, 0.2276, 0.3258],\n",
      "        [0.2304, 0.1900, 0.1280, 0.1683, 0.2833],\n",
      "        [0.2025, 0.1594, 0.1284, 0.2108, 0.2989],\n",
      "        [0.1654, 0.1252, 0.1309, 0.2249, 0.3536],\n",
      "        [0.2267, 0.1758, 0.1417, 0.2043, 0.2515]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|         | 72/8000 [00:02<03:44, 35.27batch/s, train loss (batch)=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1781, 0.1684, 0.1392, 0.2061, 0.3082],\n",
      "        [0.1591, 0.1594, 0.1384, 0.2435, 0.2997],\n",
      "        [0.1696, 0.1539, 0.1480, 0.2693, 0.2593],\n",
      "        [0.1141, 0.0857, 0.1230, 0.2613, 0.4159],\n",
      "        [0.1916, 0.1934, 0.1529, 0.2452, 0.2168],\n",
      "        [0.1492, 0.1528, 0.1325, 0.2591, 0.3064],\n",
      "        [0.1875, 0.1665, 0.1511, 0.2465, 0.2484],\n",
      "        [0.1651, 0.1535, 0.1540, 0.2354, 0.2920]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n",
      "tensor([[0.1203, 0.1680, 0.1513, 0.2611, 0.2993],\n",
      "        [0.1436, 0.1280, 0.1130, 0.2652, 0.3502],\n",
      "        [0.1850, 0.1346, 0.1632, 0.2432, 0.2740],\n",
      "        [0.1278, 0.1371, 0.1430, 0.2651, 0.3270],\n",
      "        [0.1852, 0.2222, 0.1265, 0.3034, 0.1628],\n",
      "        [0.1308, 0.1733, 0.1208, 0.2968, 0.2783],\n",
      "        [0.1461, 0.1408, 0.1479, 0.2784, 0.2869],\n",
      "        [0.0797, 0.0933, 0.1222, 0.2544, 0.4505]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|         | 88/8000 [00:02<04:14, 31.05batch/s, train loss (batch)=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1418, 0.1463, 0.1776, 0.3052, 0.2291],\n",
      "        [0.0864, 0.1042, 0.1237, 0.2643, 0.4214],\n",
      "        [0.1233, 0.1143, 0.1418, 0.2853, 0.3352],\n",
      "        [0.1344, 0.1376, 0.1421, 0.2517, 0.3341],\n",
      "        [0.1151, 0.1519, 0.1548, 0.3081, 0.2701],\n",
      "        [0.0878, 0.1013, 0.1383, 0.3083, 0.3644],\n",
      "        [0.0738, 0.0690, 0.1066, 0.2374, 0.5133],\n",
      "        [0.1444, 0.1600, 0.1528, 0.2815, 0.2612]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n",
      "tensor([[0.1166, 0.1248, 0.1507, 0.2494, 0.3584],\n",
      "        [0.0808, 0.1452, 0.1336, 0.2071, 0.4333],\n",
      "        [0.1195, 0.1324, 0.1564, 0.2191, 0.3725],\n",
      "        [0.1101, 0.1044, 0.1171, 0.2186, 0.4499],\n",
      "        [0.0781, 0.0837, 0.1214, 0.1946, 0.5222],\n",
      "        [0.1354, 0.1535, 0.1504, 0.2279, 0.3329],\n",
      "        [0.1054, 0.1104, 0.1212, 0.2344, 0.4286],\n",
      "        [0.1027, 0.1113, 0.1441, 0.2199, 0.4219]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|        | 104/8000 [00:03<04:11, 31.42batch/s, train loss (batch)=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0553, 0.0538, 0.0896, 0.1232, 0.6781],\n",
      "        [0.0449, 0.0540, 0.0889, 0.1254, 0.6867],\n",
      "        [0.1002, 0.0787, 0.1302, 0.1849, 0.5060],\n",
      "        [0.1145, 0.0854, 0.1407, 0.1689, 0.4906],\n",
      "        [0.0505, 0.0512, 0.0839, 0.0968, 0.7175],\n",
      "        [0.0494, 0.0556, 0.0936, 0.1182, 0.6832],\n",
      "        [0.1379, 0.1532, 0.1454, 0.2179, 0.3457],\n",
      "        [0.0740, 0.0760, 0.1077, 0.1426, 0.5998]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   1%|        | 112/8000 [00:03<04:27, 29.49batch/s, train loss (batch)=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0957, 0.0927, 0.1241, 0.1510, 0.5365],\n",
      "        [0.1578, 0.1639, 0.1788, 0.1814, 0.3181],\n",
      "        [0.0735, 0.1007, 0.1314, 0.1375, 0.5570],\n",
      "        [0.1054, 0.1151, 0.1615, 0.1782, 0.4398],\n",
      "        [0.0392, 0.0427, 0.0796, 0.0886, 0.7498],\n",
      "        [0.1554, 0.1230, 0.1345, 0.1323, 0.4548],\n",
      "        [0.1164, 0.0988, 0.1291, 0.1356, 0.5201],\n",
      "        [0.0476, 0.0476, 0.0817, 0.0891, 0.7340]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|        | 120/8000 [00:04<04:34, 28.71batch/s, train loss (batch)=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0579, 0.0639, 0.1114, 0.0973, 0.6695],\n",
      "        [0.1299, 0.1190, 0.1607, 0.1240, 0.4664],\n",
      "        [0.1377, 0.1234, 0.1680, 0.1373, 0.4336],\n",
      "        [0.1040, 0.1062, 0.1360, 0.1200, 0.5337],\n",
      "        [0.0406, 0.0499, 0.0921, 0.0685, 0.7489],\n",
      "        [0.1385, 0.1238, 0.1422, 0.1201, 0.4754],\n",
      "        [0.0693, 0.0712, 0.1071, 0.0922, 0.6602],\n",
      "        [0.0841, 0.0569, 0.1248, 0.0926, 0.6416]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|▏       | 128/8000 [00:04<04:31, 29.05batch/s, train loss (batch)=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1497, 0.1228, 0.1678, 0.1395, 0.4201],\n",
      "        [0.0280, 0.0337, 0.0609, 0.0578, 0.8196],\n",
      "        [0.1097, 0.0907, 0.1382, 0.1181, 0.5434],\n",
      "        [0.0913, 0.0830, 0.1266, 0.1198, 0.5793],\n",
      "        [0.1698, 0.1065, 0.1841, 0.1307, 0.4089],\n",
      "        [0.1442, 0.1325, 0.1573, 0.1703, 0.3956],\n",
      "        [0.1887, 0.1597, 0.1661, 0.1337, 0.3518],\n",
      "        [0.1314, 0.1154, 0.1472, 0.1223, 0.4837]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n",
      "tensor([[0.2132, 0.1740, 0.1902, 0.1561, 0.2664],\n",
      "        [0.1638, 0.1258, 0.1741, 0.1641, 0.3722],\n",
      "        [0.1977, 0.1048, 0.1721, 0.1462, 0.3791],\n",
      "        [0.1930, 0.1868, 0.1915, 0.1686, 0.2601],\n",
      "        [0.1855, 0.1382, 0.1730, 0.1416, 0.3618],\n",
      "        [0.1574, 0.1171, 0.1781, 0.1369, 0.4105],\n",
      "        [0.1300, 0.0654, 0.1666, 0.0994, 0.5386],\n",
      "        [0.0704, 0.0771, 0.1463, 0.1274, 0.5787]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|▏       | 136/8000 [00:04<05:04, 25.79batch/s, train loss (batch)=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1097, 0.0861, 0.2027, 0.1727, 0.4289],\n",
      "        [0.2011, 0.1552, 0.2654, 0.1844, 0.1940],\n",
      "        [0.1961, 0.1696, 0.2318, 0.1826, 0.2200],\n",
      "        [0.2096, 0.2059, 0.1731, 0.1571, 0.2543],\n",
      "        [0.1867, 0.1853, 0.2083, 0.1486, 0.2711],\n",
      "        [0.1402, 0.1100, 0.1776, 0.1489, 0.4233],\n",
      "        [0.1964, 0.2226, 0.1706, 0.1662, 0.2442],\n",
      "        [0.2127, 0.2058, 0.2001, 0.1721, 0.2094]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|▏       | 152/8000 [00:05<04:56, 26.49batch/s, train loss (batch)=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2488, 0.2031, 0.2044, 0.1726, 0.1710],\n",
      "        [0.1531, 0.1209, 0.2024, 0.1717, 0.3519],\n",
      "        [0.1570, 0.1291, 0.2076, 0.1853, 0.3211],\n",
      "        [0.1991, 0.2345, 0.1749, 0.1766, 0.2149],\n",
      "        [0.1752, 0.1184, 0.2622, 0.2210, 0.2233],\n",
      "        [0.1791, 0.1228, 0.1611, 0.1944, 0.3427],\n",
      "        [0.2086, 0.1862, 0.1912, 0.2003, 0.2137],\n",
      "        [0.2251, 0.2024, 0.2113, 0.2174, 0.1438]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|▏       | 160/8000 [00:05<04:31, 28.84batch/s, train loss (batch)=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1816, 0.0977, 0.2273, 0.2609, 0.2324],\n",
      "        [0.2352, 0.2223, 0.1996, 0.1402, 0.2027],\n",
      "        [0.2022, 0.1844, 0.2156, 0.1921, 0.2057],\n",
      "        [0.1535, 0.1674, 0.2508, 0.2459, 0.1825],\n",
      "        [0.2287, 0.1659, 0.1975, 0.1926, 0.2153],\n",
      "        [0.2160, 0.1902, 0.1910, 0.2214, 0.1814],\n",
      "        [0.1941, 0.1434, 0.1888, 0.1602, 0.3135],\n",
      "        [0.1881, 0.1174, 0.1999, 0.1862, 0.3085]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n",
      "tensor([[0.2023, 0.1816, 0.1831, 0.1979, 0.2352],\n",
      "        [0.1640, 0.1268, 0.1910, 0.2397, 0.2785],\n",
      "        [0.2621, 0.1833, 0.1806, 0.2130, 0.1610],\n",
      "        [0.2316, 0.1643, 0.1836, 0.2323, 0.1883],\n",
      "        [0.2249, 0.1267, 0.2038, 0.2352, 0.2094],\n",
      "        [0.1845, 0.0920, 0.1646, 0.2723, 0.2866],\n",
      "        [0.2654, 0.1701, 0.2282, 0.2004, 0.1359],\n",
      "        [0.0923, 0.0914, 0.2012, 0.2731, 0.3420]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|▏       | 176/8000 [00:06<04:35, 28.36batch/s, train loss (batch)=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1453, 0.1557, 0.2231, 0.2640, 0.2119],\n",
      "        [0.2408, 0.1931, 0.1552, 0.2635, 0.1475],\n",
      "        [0.1794, 0.1362, 0.1834, 0.2537, 0.2472],\n",
      "        [0.1404, 0.1544, 0.2452, 0.2670, 0.1929],\n",
      "        [0.1770, 0.1266, 0.1560, 0.2694, 0.2709],\n",
      "        [0.1979, 0.1837, 0.1638, 0.2499, 0.2047],\n",
      "        [0.2289, 0.1664, 0.1858, 0.2440, 0.1750],\n",
      "        [0.0755, 0.0761, 0.1650, 0.2813, 0.4021]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:   2%|▏       | 184/8000 [00:06<04:01, 32.39batch/s, train loss (batch)=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0971, 0.0785, 0.1115, 0.2109, 0.5020],\n",
      "        [0.1740, 0.1228, 0.1519, 0.2847, 0.2666],\n",
      "        [0.1795, 0.1646, 0.1748, 0.2477, 0.2334],\n",
      "        [0.1983, 0.1154, 0.1863, 0.3391, 0.1609],\n",
      "        [0.1698, 0.0964, 0.1590, 0.3187, 0.2561],\n",
      "        [0.1219, 0.0974, 0.1502, 0.2248, 0.4056],\n",
      "        [0.1556, 0.1099, 0.1606, 0.2635, 0.3105],\n",
      "        [0.2269, 0.1545, 0.1541, 0.2407, 0.2238]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epoch 1/100:   2%|▏       | 184/8000 [00:06<04:40, 27.84batch/s, train loss (batch)=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0566, 0.0656, 0.1299, 0.2959, 0.4520],\n",
      "        [0.1546, 0.1107, 0.1322, 0.3111, 0.2913],\n",
      "        [0.1350, 0.1075, 0.1531, 0.3864, 0.2179],\n",
      "        [0.1282, 0.0790, 0.1024, 0.3029, 0.3875],\n",
      "        [0.1410, 0.1121, 0.1214, 0.3010, 0.3246],\n",
      "        [0.1782, 0.1438, 0.1364, 0.3133, 0.2283],\n",
      "        [0.1979, 0.1533, 0.1659, 0.2611, 0.2218],\n",
      "        [0.1788, 0.0928, 0.1386, 0.2809, 0.3088]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1259644/3202099471.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/NLP-brain-biased-robustness/nlpbbb/training_loops.py\u001b[0m in \u001b[0;36mrun_training_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m#save whenever you validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_frequency\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/NLP-brain-biased-robustness/nlpbbb/training_loops.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_config, exp, epoch, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# standard pytorch backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpbbb/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpbbb/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpbbb/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpbbb/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpbbb/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bbb.training_loops.run_training_config(run_dicts[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbbb",
   "language": "python",
   "name": "nlpbbb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
