{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb884dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba539ef8",
   "metadata": {},
   "source": [
    "Download here: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67128d51",
   "metadata": {},
   "source": [
    "# Make splits (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e431e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = '/home/ubuntu/NLP-brain-biased-robustness/data/yelp/'\n",
    "f1 = open(data_path+'yelp_academic_dataset_business.json') #150346\n",
    "f2 = open(data_path+'yelp_academic_dataset_review.json') #6990280\n",
    "\n",
    "business = []\n",
    "for line in f1:\n",
    "    business.append(json.loads(line))\n",
    "\n",
    "review = []\n",
    "for line in f2:\n",
    "    review.append(json.loads(line))\n",
    "\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531056ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_business_ids = []\n",
    "japanese_business_ids = []\n",
    "chinese_business_ids = []\n",
    "italian_business_ids = []\n",
    "\n",
    "for example in business:\n",
    "    if (not example['categories'] is None) and 'American' in example['categories']:\n",
    "        american_business_ids.append(example['business_id'])\n",
    "    if (not example['categories'] is None) and 'Japanese' in example['categories']:\n",
    "        japanese_business_ids.append(example['business_id'])\n",
    "    if (not example['categories'] is None) and 'Chinese' in example['categories']:\n",
    "        chinese_business_ids.append(example['business_id'])\n",
    "    if (not example['categories'] is None) and 'Italian' in example['categories']:\n",
    "        italian_business_ids.append(example['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486493a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13066\n",
      "1830\n",
      "4573\n",
      "3343\n"
     ]
    }
   ],
   "source": [
    "print(len(american_business_ids))\n",
    "print(len(japanese_business_ids))\n",
    "print(len(italian_business_ids))\n",
    "print(len(chinese_business_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a10c43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3.5595893859863284e-05\n",
      "250000\n",
      "3.8109967788060506\n",
      "500000\n",
      "7.7034997542699175\n",
      "750000\n",
      "11.659009126822154\n",
      "1000000\n",
      "15.495580875873566\n",
      "1250000\n",
      "19.41890575091044\n",
      "1500000\n",
      "23.406390301386516\n",
      "1750000\n",
      "27.311221476395925\n",
      "2000000\n",
      "31.324174058437347\n",
      "2250000\n",
      "35.39449222485224\n",
      "2500000\n",
      "39.39674288034439\n",
      "2750000\n",
      "43.51244024435679\n",
      "3000000\n",
      "47.61593271493912\n",
      "3250000\n",
      "51.7338965177536\n",
      "3500000\n",
      "55.86985433101654\n",
      "3750000\n",
      "59.9607965985934\n",
      "4000000\n",
      "64.13195392688115\n",
      "4250000\n",
      "68.31887435118357\n",
      "4500000\n",
      "72.4751049598058\n",
      "4750000\n",
      "76.7526405374209\n",
      "5000000\n",
      "80.98935148715972\n",
      "5250000\n",
      "85.24175108671189\n",
      "5500000\n",
      "89.59655721187592\n",
      "5750000\n",
      "93.96100403865178\n",
      "6000000\n",
      "98.38862371047338\n",
      "6250000\n",
      "102.86650851567586\n",
      "6500000\n",
      "107.35190171003342\n",
      "6750000\n",
      "111.7896115342776\n"
     ]
    }
   ],
   "source": [
    "#takes two hours\n",
    "\n",
    "import time\n",
    "\n",
    "american = []\n",
    "japanese = []\n",
    "chinese = []\n",
    "italian = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, example in enumerate(review):\n",
    "    if example['business_id'] in american_business_ids:\n",
    "        american.append(example)\n",
    "    if example['business_id'] in japanese_business_ids:\n",
    "        japanese.append(example)\n",
    "    if example['business_id'] in chinese_business_ids:\n",
    "        chinese.append(example)\n",
    "    if example['business_id'] in italian_business_ids:\n",
    "        italian.append(example)\n",
    "    if idx%250000 == 0:\n",
    "\n",
    "        \n",
    "with open('american.json', 'w') as f3:\n",
    "    json.dump(american, f3)\n",
    "with open('japanese.json', 'w') as f4:\n",
    "    json.dump(japanese, f4)\n",
    "with open('chinese.json', 'w') as f5:\n",
    "    json.dump(chinese, f5)\n",
    "with open('italian.json', 'w') as f6:\n",
    "    json.dump(italian, f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268a6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv american.json ~/NLP-brain-biased-robustness/data/yelp\n",
    "!mv italian.json ~/NLP-brain-biased-robustness/data/yelp\n",
    "!mv japanese.json ~/NLP-brain-biased-robustness/data/yelp\n",
    "!mv chinese.json ~/NLP-brain-biased-robustness/data/yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27680a01",
   "metadata": {},
   "source": [
    "# Pre-processing and training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a697a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = '/home/ubuntu/NLP-brain-biased-robustness/data/yelp/'\n",
    "f1 = open(data_path+'american.json')\n",
    "f2 = open(data_path+'italian.json')\n",
    "f3 = open(data_path+'japanese.json')\n",
    "f4 = open(data_path+'chinese.json')\n",
    "\n",
    "american = []\n",
    "for line in f1:\n",
    "    american.append(json.loads(line))\n",
    "\n",
    "italian = []\n",
    "for line in f2:\n",
    "    italian.append(json.loads(line))\n",
    "    \n",
    "japanese = []\n",
    "for line in f3:\n",
    "    japanese.append(json.loads(line))\n",
    "    \n",
    "chinese = []\n",
    "for line in f4:\n",
    "    chinese.append(json.loads(line))\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()\n",
    "\n",
    "\n",
    "american = american[0]\n",
    "italian = italian[0]\n",
    "japanese = japanese[0]\n",
    "chinese = chinese[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea7c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "na = []\n",
    "for i in american:\n",
    "    na.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})\n",
    "\n",
    "ni = []\n",
    "for i in italian:\n",
    "    ni.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})\n",
    "\n",
    "nj = []\n",
    "for i in japanese:\n",
    "    nj.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})\n",
    "\n",
    "nc = []\n",
    "for i in chinese:\n",
    "    nc.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8af6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_dataloader = DataLoader(na, shuffle=True, batch_size=8)\n",
    "italian_dataloader = DataLoader(ni, shuffle=True, batch_size=8)\n",
    "japanese_dataloader = DataLoader(nj, shuffle=True, batch_size=8)\n",
    "chinese_dataloader = DataLoader(nc, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba202f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class PlaceHolderBERT(nn.Module):\n",
    "    def __init__(self, num_out=5, sigmoid=False, return_CLS_representation=False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.linear = nn.Linear(768,num_out)\n",
    "        self.return_CLS_representation = return_CLS_representation\n",
    "        self.sigmoid_bool = sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def forward(self, x):\n",
    "        embeddings = self.tokenizer(x, return_tensors='pt', padding=True, truncation=True)\n",
    "        embeddings.to(self.device)\n",
    "        representations = self.bert(**embeddings).last_hidden_state\n",
    "        cls_representation = representations[:,0,:]\n",
    "        pred = self.linear(cls_representation)\n",
    "        if self.return_CLS_representation:\n",
    "            return cls_representation\n",
    "        if self.sigmoid_bool:\n",
    "            return self.sigmoid(pred)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "def train(model, dataloader, num_epochs=1): #can scrap keyword\n",
    "    #optimizer as usual\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    #learning rate scheduler\n",
    "    num_training_steps = num_epochs * len(dataloader)\n",
    "    lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    #auto logging; progress bar\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    #training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader: #tryin unpacking text from 'labels' as in model development\n",
    "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            #features = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "            preds = model(batch['text'])\n",
    "            targets = batch['labels'].float().to(device)\n",
    "            loss = loss_function(preds, targets) #replace .loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    for batch in dataloader:\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #features = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch['text'])\n",
    "            preds = torch.argmax(preds, axis=1)\n",
    "            labels = torch.argmax(batch['labels'], axis=1).to(device)\n",
    "            num_correct += (preds==labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "    return float(num_correct)/float(num_samples)*100 \n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#tokenize function\n",
    "#def tokenize_dataset(examples):\n",
    "#    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "#pre-tokenize entire dataset\n",
    "#tokenized_american = na.map(tokenize_dataset, batched=True)\n",
    "#tokenized_italian = ni.map(tokenize_dataset, batched=True)\n",
    "#tokenized_japanese = nj.map(tokenize_dataset, batched=True)\n",
    "#tokenized_chinese = nc.map(tokenize_dataset, batched=True)\n",
    "\n",
    "#tokenized_american = tokenized_american.remove_columns([\"text\"])\n",
    "#tokenized_american.set_format(\"torch\")\n",
    "#tokenized_italian = tokenized_italian.remove_columns([\"text\"])\n",
    "#tokenized_italian.set_format(\"torch\")\n",
    "#tokenized_japanese = tokenized_japanese.remove_columns([\"text\"])\n",
    "#tokenized_japanese.set_format(\"torch\")\n",
    "#tokenized_chinese = tokenized_chinese.remove_columns([\"text\"])\n",
    "#tokenized_chinese.set_format(\"torch\")\n",
    "\n",
    "### Only for practice\n",
    "#american_small = tokenized_american.shuffle(seed=42).select(range(10000))\n",
    "#italian_small = tokenized_italian.shuffle(seed=42).select(range(10000))\n",
    "#japanese_small = tokenized_japanese.shuffle(seed=42).select(range(10000))\n",
    "#chinese_small = tokenized_chinese.shuffle(seed=42).select(range(10000))\n",
    "###\n",
    "#american_dataloader = DataLoader(na, shuffle=True, batch_size=8)\n",
    "#italian_dataloader = DataLoader(ni, shuffle=True, batch_size=8)\n",
    "#japanese_dataloader = DataLoader(nj, shuffle=True, batch_size=8)\n",
    "#chinese_dataloader = DataLoader(nc, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08cd4189",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517b8d26f4274b539149020f99cb33a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27033 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PlaceHolderBERT()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjapanese_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader: \u001b[38;5;66;03m#tryin unpacking text from 'labels' as in model development\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#batch = {k: v.to(device) for k, v in batch.items()}\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#features = {k: v for k, v in batch.items() if k != 'labels'}\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 55\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(preds, targets) \u001b[38;5;66;03m#replace .loss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PlaceHolderBERT()\n",
    "train(model, japanese_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfac8d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.300000000000004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
