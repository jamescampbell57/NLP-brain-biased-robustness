{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb884dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba539ef8",
   "metadata": {},
   "source": [
    "Download here: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67128d51",
   "metadata": {},
   "source": [
    "# Make splits (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e431e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = f'{PATHS[\"root\"]}/data/yelp/'\n",
    "f1 = open(data_path+'yelp_academic_dataset_business.json') #150346\n",
    "f2 = open(data_path+'yelp_academic_dataset_review.json') #6990280\n",
    "\n",
    "business = []\n",
    "for line in f1:\n",
    "    business.append(json.loads(line))\n",
    "\n",
    "review = []\n",
    "for line in f2:\n",
    "    review.append(json.loads(line))\n",
    "\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531056ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_business_ids = []\n",
    "japanese_business_ids = []\n",
    "chinese_business_ids = []\n",
    "italian_business_ids = []\n",
    "\n",
    "for example in business:\n",
    "    if (not example['categories'] is None) and 'American' in example['categories']:\n",
    "        american_business_ids.append(example['business_id'])\n",
    "    if (not example['categories'] is None) and 'Japanese' in example['categories']:\n",
    "        japanese_business_ids.append(example['business_id'])\n",
    "    if (not example['categories'] is None) and 'Chinese' in example['categories']:\n",
    "        chinese_business_ids.append(example['business_id'])\n",
    "    if (not example['categories'] is None) and 'Italian' in example['categories']:\n",
    "        italian_business_ids.append(example['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486493a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13066\n",
      "1830\n",
      "4573\n",
      "3343\n"
     ]
    }
   ],
   "source": [
    "print(len(american_business_ids))\n",
    "print(len(japanese_business_ids))\n",
    "print(len(italian_business_ids))\n",
    "print(len(chinese_business_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a10c43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3.5595893859863284e-05\n",
      "250000\n",
      "3.8109967788060506\n",
      "500000\n",
      "7.7034997542699175\n",
      "750000\n",
      "11.659009126822154\n",
      "1000000\n",
      "15.495580875873566\n",
      "1250000\n",
      "19.41890575091044\n",
      "1500000\n",
      "23.406390301386516\n",
      "1750000\n",
      "27.311221476395925\n",
      "2000000\n",
      "31.324174058437347\n",
      "2250000\n",
      "35.39449222485224\n",
      "2500000\n",
      "39.39674288034439\n",
      "2750000\n",
      "43.51244024435679\n",
      "3000000\n",
      "47.61593271493912\n",
      "3250000\n",
      "51.7338965177536\n",
      "3500000\n",
      "55.86985433101654\n",
      "3750000\n",
      "59.9607965985934\n",
      "4000000\n",
      "64.13195392688115\n",
      "4250000\n",
      "68.31887435118357\n",
      "4500000\n",
      "72.4751049598058\n",
      "4750000\n",
      "76.7526405374209\n",
      "5000000\n",
      "80.98935148715972\n",
      "5250000\n",
      "85.24175108671189\n",
      "5500000\n",
      "89.59655721187592\n",
      "5750000\n",
      "93.96100403865178\n",
      "6000000\n",
      "98.38862371047338\n",
      "6250000\n",
      "102.86650851567586\n",
      "6500000\n",
      "107.35190171003342\n",
      "6750000\n",
      "111.7896115342776\n"
     ]
    }
   ],
   "source": [
    "#takes two hours\n",
    "\n",
    "import time\n",
    "\n",
    "american = []\n",
    "japanese = []\n",
    "chinese = []\n",
    "italian = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, example in enumerate(review):\n",
    "    if example['business_id'] in american_business_ids:\n",
    "        american.append(example)\n",
    "    if example['business_id'] in japanese_business_ids:\n",
    "        japanese.append(example)\n",
    "    if example['business_id'] in chinese_business_ids:\n",
    "        chinese.append(example)\n",
    "    if example['business_id'] in italian_business_ids:\n",
    "        italian.append(example)\n",
    "\n",
    "        \n",
    "with open('american.json', 'w') as f3:\n",
    "    json.dump(american, f3)\n",
    "with open('japanese.json', 'w') as f4:\n",
    "    json.dump(japanese, f4)\n",
    "with open('chinese.json', 'w') as f5:\n",
    "    json.dump(chinese, f5)\n",
    "with open('italian.json', 'w') as f6:\n",
    "    json.dump(italian, f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268a6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv american.json ~/NLP-brain-biased-robustness/data/yelp\n",
    "!mv italian.json ~/NLP-brain-biased-robustness/data/yelp\n",
    "!mv japanese.json ~/NLP-brain-biased-robustness/data/yelp\n",
    "!mv chinese.json ~/NLP-brain-biased-robustness/data/yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27680a01",
   "metadata": {},
   "source": [
    "# Pre-processing and training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a697a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = '/home/ubuntu/NLP-brain-biased-robustness/data/yelp/'\n",
    "f1 = open(data_path+'american.json')\n",
    "f2 = open(data_path+'italian.json')\n",
    "f3 = open(data_path+'japanese.json')\n",
    "f4 = open(data_path+'chinese.json')\n",
    "\n",
    "american = []\n",
    "for line in f1:\n",
    "    american.append(json.loads(line))\n",
    "\n",
    "italian = []\n",
    "for line in f2:\n",
    "    italian.append(json.loads(line))\n",
    "    \n",
    "japanese = []\n",
    "for line in f3:\n",
    "    japanese.append(json.loads(line))\n",
    "    \n",
    "chinese = []\n",
    "for line in f4:\n",
    "    chinese.append(json.loads(line))\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()\n",
    "\n",
    "\n",
    "american = american[0]\n",
    "italian = italian[0]\n",
    "japanese = japanese[0]\n",
    "chinese = chinese[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6bb974",
   "metadata": {},
   "outputs": [],
   "source": [
    "american = american[:10000]\n",
    "italian = italian[:10000]\n",
    "japanese = japanese[:10000]\n",
    "chinese = chinese[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea7c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "na = []\n",
    "for i in american:\n",
    "    na.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})\n",
    "\n",
    "ni = []\n",
    "for i in italian:\n",
    "    ni.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})\n",
    "\n",
    "nj = []\n",
    "for i in japanese:\n",
    "    nj.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})\n",
    "\n",
    "nc = []\n",
    "for i in chinese:\n",
    "    nc.append({'text': i['text'], 'labels': F.one_hot((torch.tensor(i['stars']-1)).to(torch.int64), num_classes=5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8af6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_dataloader = DataLoader(na, shuffle=True, batch_size=8)\n",
    "italian_dataloader = DataLoader(ni, shuffle=True, batch_size=8)\n",
    "japanese_dataloader = DataLoader(nj, shuffle=True, batch_size=8)\n",
    "chinese_dataloader = DataLoader(nc, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba202f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#!WANDB_START_METHOD = \"thread\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "def change_all_keys(pre_odict):\n",
    "    def change_key(odict, old, new):\n",
    "        for _ in range(len(odict)):\n",
    "            k, v = odict.popitem(False)\n",
    "            odict[new if old == k else k] = v\n",
    "            return odict\n",
    "    for key in pre_odict.keys():\n",
    "        if key[:5] == 'bert.':\n",
    "            post_odict = change_key(pre_odict, key, key[5:])\n",
    "            return change_all_keys(post_odict)\n",
    "        if key[:7] == 'linear.':\n",
    "            del pre_odict[key]\n",
    "            return change_all_keys(pre_odict)\n",
    "    return pre_odict\n",
    "\n",
    "class PlaceHolderBERT(nn.Module):\n",
    "    def __init__(self, num_out=5, sigmoid=False, return_CLS_representation=False, brain=True):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        if brain:\n",
    "            state_path = '/home/ubuntu/NLP-brain-biased-robustness/state_dicts/fine_tuned_model'\n",
    "            pre_odict = torch.load(state_path)\n",
    "            filtered_odict = change_all_keys(pre_odict)\n",
    "            self.bert.load_state_dict(filtered_odict, strict=True)\n",
    "        self.linear = nn.Linear(768,num_out)\n",
    "        self.return_CLS_representation = return_CLS_representation\n",
    "        self.sigmoid_bool = sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def forward(self, x):\n",
    "        embeddings = self.tokenizer(x, return_tensors='pt', padding=True, truncation=True)\n",
    "        embeddings.to(self.device)\n",
    "        representations = self.bert(**embeddings).last_hidden_state\n",
    "        cls_representation = representations[:,0,:]\n",
    "        pred = self.linear(cls_representation)\n",
    "        if self.return_CLS_representation:\n",
    "            return cls_representation\n",
    "        if self.sigmoid_bool:\n",
    "            return self.sigmoid(pred)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "def train(model, dataloader, num_epochs=10): #can scrap keyword\n",
    "    wandb.require(experiment=\"service\")\n",
    "    wandb.init(project=\"preliminary results just in case\", entity=\"nlp-brain-biased-robustness\")\n",
    "    wandb.run.name = 'yelp bert 10 epochs'\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": 5e-5,\n",
    "      \"epochs\": 10,\n",
    "      \"batch_size\": 8\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #optimizer as usual\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    #learning rate scheduler\n",
    "    num_training_steps = num_epochs * len(dataloader)\n",
    "    lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    #auto logging; progress bar\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    #training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader: #tryin unpacking text from 'labels' as in model development\n",
    "            #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            #features = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "            preds = model(batch['text'])\n",
    "            targets = batch['labels'].float().to(device)\n",
    "            loss = loss_function(preds, targets) #replace .loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        american_score = evaluate(model, american_dataloader)\n",
    "        wandb.log({\"american\": american_score})\n",
    "        italian_score = evaluate(model, italian_dataloader)\n",
    "        wandb.log({\"italian\": italian_score})\n",
    "        japanese_score = evaluate(model, japanese_dataloader)\n",
    "        wandb.log({\"japanese\": japanese_score})\n",
    "        chinese_score = evaluate(model, chinese_dataloader)\n",
    "        wandb.log({\"chinese\": chinese_score})\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    for batch in dataloader:\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #features = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch['text'])\n",
    "            preds = torch.argmax(preds, axis=1)\n",
    "            labels = torch.argmax(batch['labels'], axis=1).to(device)\n",
    "            num_correct += (preds==labels).sum()\n",
    "            num_samples += preds.size(0)\n",
    "    return float(num_correct)/float(num_samples)*100 \n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#tokenize function\n",
    "#def tokenize_dataset(examples):\n",
    "#    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "#pre-tokenize entire dataset\n",
    "#tokenized_american = na.map(tokenize_dataset, batched=True)\n",
    "#tokenized_italian = ni.map(tokenize_dataset, batched=True)\n",
    "#tokenized_japanese = nj.map(tokenize_dataset, batched=True)\n",
    "#tokenized_chinese = nc.map(tokenize_dataset, batched=True)\n",
    "\n",
    "#tokenized_american = tokenized_american.remove_columns([\"text\"])\n",
    "#tokenized_american.set_format(\"torch\")\n",
    "#tokenized_italian = tokenized_italian.remove_columns([\"text\"])\n",
    "#tokenized_italian.set_format(\"torch\")\n",
    "#tokenized_japanese = tokenized_japanese.remove_columns([\"text\"])\n",
    "#tokenized_japanese.set_format(\"torch\")\n",
    "#tokenized_chinese = tokenized_chinese.remove_columns([\"text\"])\n",
    "#tokenized_chinese.set_format(\"torch\")\n",
    "\n",
    "### Only for practice\n",
    "#american_small = tokenized_american.shuffle(seed=42).select(range(10000))\n",
    "#italian_small = tokenized_italian.shuffle(seed=42).select(range(10000))\n",
    "#japanese_small = tokenized_japanese.shuffle(seed=42).select(range(10000))\n",
    "#chinese_small = tokenized_chinese.shuffle(seed=42).select(range(10000))\n",
    "###\n",
    "#american_dataloader = DataLoader(na, shuffle=True, batch_size=8)\n",
    "#italian_dataloader = DataLoader(ni, shuffle=True, batch_size=8)\n",
    "#japanese_dataloader = DataLoader(nj, shuffle=True, batch_size=8)\n",
    "#chinese_dataloader = DataLoader(nc, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08cd4189",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjgc239\u001b[0m (\u001b[33mnlp-brain-biased-robustness\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/NLP-brain-biased-robustness/evaluation/wandb/run-20220522_120030-nzg7zou8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp-brain-biased-robustness/preliminary%20results%20just%20in%20case/runs/nzg7zou8\" target=\"_blank\">swift-meadow-21</a></strong> to <a href=\"https://wandb.ai/nlp-brain-biased-robustness/preliminary%20results%20just%20in%20case\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8082f8704cd74cdfb458377e79505584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PlaceHolderBERT()\n",
    "train(model, italian_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfac8d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.400000000000006\n",
      "40.39\n",
      "41.61\n",
      "35.49\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, italian_dataloader))\n",
    "print(evaluate(model, american_dataloader))\n",
    "print(evaluate(model, japanese_dataloader))\n",
    "print(evaluate(model, chinese_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
